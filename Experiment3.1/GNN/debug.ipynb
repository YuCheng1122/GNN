{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd9f043",
   "metadata": {},
   "source": [
    "Check the structure of Gpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cce043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圖類型： <class 'networkx.classes.digraph.DiGraph'>\n",
      "節點數量： 341\n",
      "邊數量： 650\n",
      "節點 0x80d4L：{'pcode': ['RETURN', 'CAST']}\n",
      "節點 0x80f0L：{'pcode': ['CALLIND', 'RETURN']}\n",
      "節點 0x80f4L：{'pcode': ['CALLIND', 'RETURN']}\n",
      "節點 0x80f8L：{'pcode': ['CALLIND', 'RETURN']}\n",
      "節點 0x80fcL：{'pcode': ['CALLIND', 'RETURN']}\n",
      "✅ 是有向圖 (DiGraph)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# 目標檔案路徑\n",
    "gpickle_path = \"/home/tommy/Projects/cross-architecture/Gpickle/20250509_new_train_450_token/00/00a6f39a8f7b14f223fa51a9a23aa110112a524799e910e321b162847a875593.gpickle\"\n",
    "\n",
    "with open(gpickle_path, 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# 印出基本資訊\n",
    "print(\"圖類型：\", type(G))\n",
    "print(\"節點數量：\", G.number_of_nodes())\n",
    "print(\"邊數量：\", G.number_of_edges())\n",
    "\n",
    "# 印出前幾個節點和其屬性\n",
    "for i, (node, data) in enumerate(G.nodes(data=True)):\n",
    "    print(f\"節點 {node}：{data}\")\n",
    "    if i >= 4:  # 只看前5個節點\n",
    "        break\n",
    "\n",
    "# 檢查是否為有向圖\n",
    "if isinstance(G, nx.DiGraph):\n",
    "    print(\"✅ 是有向圖 (DiGraph)\")\n",
    "else:\n",
    "    print(\"⚠️ 不是 DiGraph，可能結構錯誤\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a67b9",
   "metadata": {},
   "source": [
    "Empty Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_attr_nodes = [node for node, data in G.nodes(data=True) if not data]\n",
    "print(f\"沒有屬性的節點數量：{len(empty_attr_nodes)}\")\n",
    "if empty_attr_nodes:\n",
    "    print(\"⚠️ 以下節點沒有屬性：\", empty_attr_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19015a0",
   "metadata": {},
   "source": [
    "Clean Gpickle Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a696e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from networkx import DiGraph    \n",
    "import networkx as nx\n",
    "\n",
    "def read_csv(csv_file_path: str | Path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    file_names = df['file_name'].tolist()\n",
    "    return file_names\n",
    "\n",
    "def clean_data(json_data, G_raw: nx.DiGraph) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for node in G_raw.nodes():\n",
    "        addr = str(node)\n",
    "        func = json_data.get(addr)\n",
    "        if not func:\n",
    "            continue\n",
    "\n",
    "        instructions = func.get(\"instructions\", [])\n",
    "        pcode_list = [p for instr in instructions if isinstance(instr, dict)\n",
    "                      for p in instr.get(\"pcode\", [])]\n",
    "\n",
    "        if pcode_list:\n",
    "            G.add_node(addr, pcode=pcode_list)\n",
    "    for src, dst in G_raw.edges():\n",
    "        src, dst = str(src), str(dst)\n",
    "        if G.has_node(src) and G.has_node(dst):\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "    return G\n",
    "\n",
    "def process_single_file_data(file_info, output_base_path):\n",
    "    json_path, dot_path, file_name = file_info\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        G_raw = nx.drawing.nx_pydot.read_dot(dot_path)\n",
    "        G = clean_data(json_data, G_raw)\n",
    "        \n",
    "        # Prepare output path\n",
    "        prefix = file_name[:2]\n",
    "        output_dir = output_base_path / prefix\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_file = output_dir / f\"{file_name}.gpickle\"\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'wb') as f:\n",
    "                pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving graph to {output_file}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Clear variables to free memory\n",
    "        del json_data, G_raw, G\n",
    "        \n",
    "        return f\"Successfully processed {file_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {file_name}: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "csv_file_path = \"/home/tommy/Projects/cross-architecture/Experiment3.1/dataset/cleaned_20250509_test_600.csv\" \n",
    "root_dir = \"/home/tommy/Projects/cross-architecture/reverse/output_new/results\" \n",
    "output_base_dir = \"/home/tommy/Projects/cross-architecture/Gpickle/20250509_new_test_600\"\n",
    "\n",
    "root_dir = Path(root_dir)\n",
    "output_base_path = Path(output_base_dir)\n",
    "file_info_list = []\n",
    "file_names = read_csv(csv_file_path)\n",
    "num_processes= None \n",
    "for file_name in tqdm(file_names, desc=\"Collecting file paths\"):\n",
    "    json_path = root_dir / file_name / f\"{file_name}.json\"\n",
    "    dot_path = root_dir / file_name / f\"{file_name}.dot\"\n",
    "    \n",
    "    if json_path.exists() and dot_path.exists():\n",
    "        file_info_list.append((json_path, dot_path, file_name))\n",
    "    else:\n",
    "        missing = []\n",
    "        if not json_path.exists():\n",
    "            missing.append(\"JSON\")\n",
    "        if not dot_path.exists():\n",
    "            missing.append(\"DOT\")\n",
    "        print(f\"Missing {', '.join(missing)} file(s) for: {file_name}\")\n",
    "\n",
    "# Use multiprocessing to process files in parallel\n",
    "if num_processes is None:\n",
    "    num_processes = mp.cpu_count()\n",
    "\n",
    "print(f\"Processing {len(file_info_list)} files using {num_processes} processes...\")\n",
    "\n",
    "# Create a partial function with fixed output_base_path\n",
    "process_func = partial(process_single_file_data, output_base_path=output_base_path)\n",
    "\n",
    "# Use multiprocessing pool to process files\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    results = list(tqdm(\n",
    "        pool.imap(process_func, file_info_list),\n",
    "        total=len(file_info_list),\n",
    "        desc=\"Processing files\"\n",
    "    ))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-architecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
