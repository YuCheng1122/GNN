{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c884e0e",
   "metadata": {},
   "source": [
    "Merge CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb52308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總行數: 33197\n",
      "重複檔名數量: 266\n",
      "\n",
      "前5個重複檔名:\n",
      "7357069bf5306677330773a936078cb4840de493fd673ef11be8843a524bdb6d: 出現 2 次\n",
      "73c581435280b4f823b5eaf90aea9cfe4999301e9852af7a1e95e9e77551434c: 出現 2 次\n",
      "73fd677a62e465c6b6f028eeda0c5f79147eff45b98478cb97864835f58a553e: 出現 2 次\n",
      "735ba90862662fd7cdc80d1ec0d440967b078fefc700e303ec083f0e57aafbe7: 出現 2 次\n",
      "737ad21399108c65f7f8b3085410f5aa0957995df4c3dbfbc2fbc72f9bd81ac5: 出現 2 次\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2486041/228864410.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduped = combined.groupby('file_name').apply(select_priority_row).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "移除重複後行數: 32931\n",
      "已儲存至 merged_deduped.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# 檔案路徑\n",
    "files = [\n",
    "    \"/home/tommy/Projects/cross-architecture/datasets/csv/deduplicated/20250504_dedup_combined_file_features.csv\",\n",
    "    \"/home/tommy/Projects/cross-architecture/datasets/csv/deduplicated/20250428-5_dedup_combined_file_features.csv\",\n",
    "    \"/home/tommy/Projects/cross-architecture/datasets/csv/deduplicated/20250430-5_dedup_combined_file_features.csv\",\n",
    "    \"/home/tommy/Projects/cross-architecture/datasets/csv/20250508_cleaned_all_malware_file_features.csv\"\n",
    "]\n",
    "\n",
    "# 讀取並合併檔案\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 檢查重複的檔名\n",
    "filename_counts = Counter(combined[\"file_name\"])\n",
    "duplicates = {name: count for name, count in filename_counts.items() if count > 1}\n",
    "\n",
    "# 顯示重複檔名數量\n",
    "print(f\"總行數: {len(combined)}\")\n",
    "print(f\"重複檔名數量: {len(duplicates)}\")\n",
    "\n",
    "# 若有重複，顯示前5個\n",
    "if duplicates:\n",
    "    print(\"\\n前5個重複檔名:\")\n",
    "    for name, count in list(duplicates.items())[:5]:\n",
    "        print(f\"{name}: 出現 {count} 次\")\n",
    "\n",
    "# 根據條件處理重複項\n",
    "# 優先保留 CPU 和 label 欄位不為空的資料\n",
    "def select_priority_row(group):\n",
    "    # 檢查 CPU 和 label 欄位是否有值\n",
    "    has_values = ~(group['CPU'].isna() | group['label'].isna())\n",
    "    if has_values.any():\n",
    "        # 返回有值的第一筆資料\n",
    "        return group[has_values].iloc[0:1]\n",
    "    else:\n",
    "        # 若都沒有值，則保留第一筆\n",
    "        return group.iloc[0:1]\n",
    "\n",
    "# 根據 file_name 分組，並對每組應用選擇函數\n",
    "deduped = combined.groupby('file_name').apply(select_priority_row).reset_index(drop=True)\n",
    "\n",
    "# 儲存結果\n",
    "deduped.to_csv(\"merged_deduped.csv\", index=False)\n",
    "print(f\"\\n移除重複後行數: {len(deduped)}\")\n",
    "print(\"已儲存至 merged_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f0c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU by Label Counts:\n",
      "                              CPU        label  count\n",
      "0                         AArch64       gafgyt     16\n",
      "1                         AArch64        kaiji     24\n",
      "2                         AArch64  meterpreter     26\n",
      "3                         AArch64        mirai    168\n",
      "4                         AArch64     mobidash     13\n",
      "5                         AArch64      tsunami     17\n",
      "6                         AArch64        wroba     11\n",
      "7                             ARM       benign   1922\n",
      "8                             ARM       dofloo    182\n",
      "9                             ARM       gafgyt   1249\n",
      "10                            ARM       hajime     45\n",
      "11                            ARM        kaiji     93\n",
      "12                            ARM  meterpreter     70\n",
      "13                            ARM        mirai   1706\n",
      "14                            ARM     mobidash     68\n",
      "15                            ARM      tsunami    809\n",
      "16                            ARM        wroba     52\n",
      "17  Advanced Micro Devices X86-64       benign   1851\n",
      "18  Advanced Micro Devices X86-64       dofloo     49\n",
      "19  Advanced Micro Devices X86-64       gafgyt   1977\n",
      "20  Advanced Micro Devices X86-64       hajime      1\n",
      "21  Advanced Micro Devices X86-64        kaiji     54\n",
      "22  Advanced Micro Devices X86-64  meterpreter    157\n",
      "23  Advanced Micro Devices X86-64        mirai   1677\n",
      "24  Advanced Micro Devices X86-64     mobidash     12\n",
      "25  Advanced Micro Devices X86-64      tsunami   1871\n",
      "26  Advanced Micro Devices X86-64        wroba     10\n",
      "27                    Intel 80386       benign   2169\n",
      "28                    Intel 80386       dofloo    346\n",
      "29                    Intel 80386       gafgyt   1243\n",
      "30                    Intel 80386        kaiji     46\n",
      "31                    Intel 80386  meterpreter     67\n",
      "32                    Intel 80386        mirai   1642\n",
      "33                    Intel 80386     mobidash     50\n",
      "34                    Intel 80386      tsunami    767\n",
      "35                    Intel 80386        wroba     16\n",
      "36                     MIPS R3000       benign   2332\n",
      "37                     MIPS R3000       dofloo     21\n",
      "38                     MIPS R3000       gafgyt   1375\n",
      "39                     MIPS R3000        kaiji     52\n",
      "40                     MIPS R3000  meterpreter     33\n",
      "41                     MIPS R3000        mirai   1721\n",
      "42                     MIPS R3000     mobidash     76\n",
      "43                     MIPS R3000      tsunami    830\n",
      "44                     MIPS R3000        wroba     45\n",
      "45                        PowerPC       benign   2470\n",
      "46                        PowerPC       gafgyt   1112\n",
      "47                        PowerPC  meterpreter     21\n",
      "48                        PowerPC        mirai   1535\n",
      "49                        PowerPC      tsunami    295\n"
     ]
    }
   ],
   "source": [
    "# Check Merged File\n",
    "\n",
    "merged_file = pd.read_csv(\"merged_deduped.csv\")\n",
    "\n",
    "# CPU by Label\n",
    "cpu_label_counts = merged_file.groupby(['CPU', 'label']).size().reset_index(name='count')\n",
    "\n",
    "print(\"\\nCPU by Label Counts:\") \n",
    "print(cpu_label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 20201 rows, saved as merged_deduped_train.csv\n",
      "Test set: 12193 rows, saved as merged_deduped_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_cpu_data(csv_filename):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    train_cpus = ['ARM', 'Advanced Micro Devices X86-64']\n",
    "    test_cpus = ['MIPS R3000']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_df = df[df['CPU'].isin(train_cpus)]\n",
    "    test_df = df[df['CPU'].isin(test_cpus)]\n",
    "    \n",
    "    # Get base filename without extension\n",
    "    base_filename = csv_filename.split('.')[0]\n",
    "    \n",
    "    # Save the train and test CSV files\n",
    "    train_filename = f\"{base_filename}_train.csv\"\n",
    "    test_filename = f\"{base_filename}_test.csv\"\n",
    "    \n",
    "    train_df.to_csv(train_filename, index=False)\n",
    "    test_df.to_csv(test_filename, index=False)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} rows, saved as {train_filename}\")\n",
    "    print(f\"Test set: {len(test_df)} rows, saved as {test_filename}\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    train, test = split_cpu_data(\"merged_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3ec69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料共 77054 筆\n",
      "標記為additional的資料共 2549 筆\n",
      "已將篩選後的資料儲存至 /home/tommy/Projects/cross-architecture/datasets/csv/Additional_Files_20250506.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 參數設定\n",
    "original_csv_path = \"/home/tommy/Projects/cross-architecture/datasets/csv/Cross-arch_Dataset_with_Additional_20250428200830.csv\"\n",
    "output_csv_path = \"/home/tommy/Projects/cross-architecture/datasets/csv/Additional_Files_20250506.csv\"\n",
    "\n",
    "# 讀取原始CSV檔案\n",
    "df = pd.read_csv(original_csv_path)\n",
    "\n",
    "# 篩選source欄位值為additional的資料\n",
    "filtered_df = df[df['source'] == 'additional']\n",
    "\n",
    "# Drop Source欄位\n",
    "filtered_df = filtered_df.drop(columns=['source'])\n",
    "\n",
    "# 顯示篩選結果數量\n",
    "print(f\"原始資料共 {len(df)} 筆\")\n",
    "print(f\"標記為additional的資料共 {filtered_df.shape[0]} 筆\")\n",
    "\n",
    "# 儲存篩選後的資料到新CSV檔案\n",
    "filtered_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"已將篩選後的資料儲存至 {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5355d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/tommy/datasets/202403_Malware(New).csv\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-architecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
